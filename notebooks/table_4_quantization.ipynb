{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "# Enable hot autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "from utils import compute_perplexity_df\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_TOKENIZER_PATH = \"<LLAMA_TOKENIZER_PATH>\"\n",
    "LLAMA_MODEL_PATH = \"<LLAMA_MODEL_PATH>\"\n",
    "\n",
    "# pickled pd.DataFrame, produced by inject_traps.py\n",
    "TRAP_INFO_PATH = \"<TRAP_INFO_PATH>\"\n",
    "\n",
    "# We've generated additional non-member trap sequences with the scripts `gen_traps.py`\n",
    "NON_MEMBERS_PATH_TEMPLATE = \"<PATH_TO_NON_MEMBER_TRAPS_SEQ_LEN_%d>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = LlamaTokenizer.from_pretrained(LLAMA_TOKENIZER_PATH, torch_dtype=torch.float16)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "llama_model = LlamaForCausalLM.from_pretrained(LLAMA_MODEL_PATH)\n",
    "llama_model = llama_model.to(\"cuda:1\")\n",
    "\n",
    "croissant_tokenizer = AutoTokenizer.from_pretrained(\"croissantllm/base_190k\")\n",
    "croissant_tokenizer.pad_token = croissant_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAP_INFO_PATH, \"rb\") as f:\n",
    "    trap_info = pickle.load(f)\n",
    "trap_info[\"raw_traps\"] = llama_tokenizer.batch_decode(trap_info.trap_tokens)\n",
    "\n",
    "non_members = {}\n",
    "for seq_len in [25, 50, 100]:\n",
    "    nm_path = NON_MEMBERS_PATH_TEMPLATE % seq_len\n",
    "    with open(nm_path, \"rb\") as f:\n",
    "        non_members[seq_len] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_croissant_model(croissant_checkpoint: str, torch_dtype: str):\n",
    "    model_load_kwargs = {}\n",
    "    if torch_dtype == 'int8':\n",
    "        model_load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=0.0,\n",
    "        )\n",
    "    elif torch_dtype == 'int4':\n",
    "        model_load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "    elif torch_dtype == 'float32':\n",
    "        model_load_kwargs[\"torch_dtype\"] = torch.float32\n",
    "    else:\n",
    "        model_load_kwargs[\"torch_dtype\"] = torch.float16\n",
    "    \n",
    "    model_load_kwargs[\"cache_dir\"] = \"/home/igor/rds/ephemeral/.huggingface\"\n",
    "\n",
    "    if 'int' in torch_dtype:\n",
    "        model_load_kwargs[\"device_map\"] = 'auto'\n",
    "        croissant_model = AutoModelForCausalLM.from_pretrained(croissant_checkpoint, **model_load_kwargs)\n",
    "    else:\n",
    "        device = \"cuda:0\"\n",
    "        print(\"Using device:\", device)\n",
    "        croissant_model = AutoModelForCausalLM.from_pretrained(croissant_checkpoint, **model_load_kwargs).to(device)\n",
    "\n",
    "    return croissant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(df_res, seq_len, n_rep):\n",
    "    y_score, y_true = [], []\n",
    "    df_members = df_res[(df_res.seq_len == seq_len) & (df_res.n_rep == n_rep)]\n",
    "    score_members = np.log(df_members[\"croissant_ppl\"]) / np.log(df_members[\"llama_ppl\"])\n",
    "    y_score.extend(list(score_members))\n",
    "    y_true.extend([1] * len(score_members))\n",
    "\n",
    "    df_non_members = df_res[(df_res.seq_len == seq_len) & (df_res.n_rep == 0)]\n",
    "    score_non_members = np.log(df_non_members[\"croissant_ppl\"]) / np.log(df_non_members[\"llama_ppl\"])\n",
    "    y_score.extend(list(score_non_members))\n",
    "    y_true.extend([0] * len(score_non_members))\n",
    "\n",
    "    auc = roc_auc_score(y_true, -np.array(y_score))\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "ppl_set = set(trap_info.ppl_bucket)\n",
    "n_rep = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in tqdm([\"int4\",\"int8\",\"float16\",\"float32\"]):\n",
    "    croissant_model = get_croissant_model(\"croissantllm/base_190k\", dtype)\n",
    "\n",
    "    df_res = pd.DataFrame()\n",
    "\n",
    "    for ppl in ppl_set:\n",
    "        df_filter = trap_info[\n",
    "            (trap_info.seq_len == seq_len)\n",
    "            & (trap_info.ppl_bucket == ppl)\n",
    "            & (trap_info.n_rep == n_rep)\n",
    "        ]\n",
    "\n",
    "        df_res_tmp = compute_perplexity_df(\n",
    "            llama_model=llama_model,\n",
    "            croissant_model=croissant_model,\n",
    "            llama_tokenizer=llama_tokenizer,\n",
    "            croissant_tokenizer=croissant_tokenizer,\n",
    "            raw_traps=df_filter.raw_traps,\n",
    "        )\n",
    "\n",
    "        df_res_tmp[\"seq_len\"] = seq_len\n",
    "        df_res_tmp[\"ppl\"] = ppl\n",
    "        df_res_tmp[\"n_rep\"] = n_rep\n",
    "\n",
    "        df_res = pd.concat([df_res, df_res_tmp])\n",
    "\n",
    "\n",
    "    for ppl in ppl_set:\n",
    "        key = (ppl * 10 + 1, ppl * 10 + 11)\n",
    "        raw_traps = llama_tokenizer.batch_decode(non_members[seq_len][key][:, 1:])\n",
    "\n",
    "        df_res_tmp = compute_perplexity_df(\n",
    "            llama_model=llama_model,\n",
    "            croissant_model=croissant_model,\n",
    "            llama_tokenizer=llama_tokenizer,\n",
    "            croissant_tokenizer=croissant_tokenizer,\n",
    "            raw_traps=raw_traps,\n",
    "        )\n",
    "        df_res_tmp[\"seq_len\"] = seq_len\n",
    "        df_res_tmp[\"ppl\"] = ppl\n",
    "        df_res_tmp[\"n_rep\"] = 0\n",
    "\n",
    "        df_res = pd.concat([df_res, df_res_tmp])\n",
    "\n",
    "    df_res[\"ratio\"] = df_res.croissant_ppl / df_res.llama_ppl\n",
    "\n",
    "    print(f\"dtype={dtype}, AUC={compute_auc(df_res, seq_len, n_rep)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
