{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "# Enable hot autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from utils import compute_perplexity\n",
    "from datasets import load_from_disk\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_TOKENIZER_PATH = \"<LLAMA_TOKENIZER_PATH>\"\n",
    "LLAMA_MODEL_PATH = \"<LLAMA_MODEL_PATH>\"\n",
    "\n",
    "DATASET_PATH = \"<DATASET_PATH>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(LLAMA_TOKENIZER_PATH, torch_dtype=torch.float16)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(LLAMA_MODEL_PATH)\n",
    "model = model.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(DATASET_PATH)\n",
    "dataset = dataset.map(\n",
    "    lambda samples: tokenizer(samples[\"text\"]),\n",
    "    batched=False,\n",
    "    num_proc=92,\n",
    ")\n",
    "\n",
    "dataset = dataset.filter(\n",
    "    lambda x: len(x[\"input_ids\"]) > 5000,\n",
    "    num_proc=92,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = defaultdict(list)\n",
    "N_SAMPLES = 1000\n",
    "SEQ_LEN_SET = (25, 50, 100)\n",
    "\n",
    "for seq_len in SEQ_LEN_SET:\n",
    "    for _ in tqdm(range(N_SAMPLES)):\n",
    "        doc_id = random.choice(range(len(dataset)))\n",
    "        doc = dataset[doc_id]\n",
    "\n",
    "        if len(doc[\"input_ids\"]) < seq_len:\n",
    "            continue\n",
    "\n",
    "        start_idx = random.choice(range(len(doc[\"input_ids\"]) - seq_len + 1))\n",
    "        input_ids = torch.tensor(doc[\"input_ids\"][start_idx:start_idx + seq_len]).unsqueeze(0).to(\"cuda:1\")\n",
    "        attention_mask = torch.tensor(doc[\"attention_mask\"][start_idx:start_idx + seq_len]).unsqueeze(0).to(\"cuda:1\")\n",
    "\n",
    "        ppl = compute_perplexity(model, input_ids, attention_mask)\n",
    "        perplexities[seq_len].append(ppl.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bins, _ = plt.hist(perplexities[25], bins=200, density=True, alpha=0.5, label=r\"$L_{ref}=25$\", color=\"darkred\")\n",
    "plt.hist(perplexities[50], bins=bins, density=True, alpha=0.5, label=r\"$L_{ref}=50$\", color=\"darkblue\")\n",
    "plt.hist(perplexities[100], bins=bins, density=True, alpha=0.5, label=r\"$L_{ref}=100$\", color=\"darkgreen\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
