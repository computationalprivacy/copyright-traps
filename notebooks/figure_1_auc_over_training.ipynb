{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "# Enable hot autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
    "from utils import compute_perplexity_df\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_TOKENIZER_PATH = \"<LLAMA_TOKENIZER_PATH>\"\n",
    "LLAMA_MODEL_PATH = \"<LLAMA_MODEL_PATH>\"\n",
    "\n",
    "# pickled pd.DataFrame, produced by inject_traps.py\n",
    "TRAP_INFO_PATH = \"<TRAP_INFO_PATH>\"\n",
    "\n",
    "# We've generated additional non-member canaries with the scripts `gen_traps.py`\n",
    "NON_MEMBERS_PATH_TEMPLATE = \"<PATH_TO_NON_MEMBER_TRAPS_SEQ_LEN_%d>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = LlamaForCausalLM.from_pretrained(LLAMA_MODEL_PATH)\n",
    "\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(LLAMA_TOKENIZER_PATH, torch_dtype=torch.float16)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "croissant_tokenizer = AutoTokenizer.from_pretrained(\"croissantllm/base_190k\")\n",
    "croissant_tokenizer.pad_token = croissant_tokenizer.eos_token\n",
    "\n",
    "llama_model = llama_model.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAP_INFO_PATH, \"rb\") as f:\n",
    "    trap_info = pickle.load(f)\n",
    "trap_info[\"raw_canaries\"] = llama_tokenizer.batch_decode(trap_info.canary_tokens)\n",
    "\n",
    "non_members = {}\n",
    "for seq_len in [25, 50, 100]:\n",
    "    nm_path = NON_MEMBERS_PATH_TEMPLATE % seq_len\n",
    "    with open(nm_path, \"rb\") as f:\n",
    "        non_members[seq_len] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_set = set(trap_info.seq_len)\n",
    "ppl_set = set(trap_info.ppl_bucket)\n",
    "n_rep = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_res(croissant_model):\n",
    "    df_res = pd.DataFrame()\n",
    "\n",
    "    for seq_len, ppl in tqdm(list(itertools.product(seq_len_set, ppl_set))):\n",
    "        df_filter = trap_info[\n",
    "            (trap_info.seq_len == seq_len)\n",
    "            & (trap_info.ppl_bucket == ppl)\n",
    "            & (trap_info.n_rep == n_rep)\n",
    "        ]\n",
    "\n",
    "        df_res_tmp = compute_perplexity_df(\n",
    "            llama_model=llama_model,\n",
    "            croissant_model=croissant_model,\n",
    "            llama_tokenizer=llama_tokenizer,\n",
    "            croissant_tokenizer=croissant_tokenizer,\n",
    "            raw_canaries=df_filter.raw_canaries,\n",
    "        )\n",
    "\n",
    "        df_res_tmp[\"seq_len\"] = seq_len\n",
    "        df_res_tmp[\"ppl\"] = ppl\n",
    "        df_res_tmp[\"n_rep\"] = n_rep\n",
    "\n",
    "        df_res = pd.concat([df_res, df_res_tmp])\n",
    "\n",
    "\n",
    "    for seq_len, ppl in tqdm(list(itertools.product(seq_len_set, ppl_set))):\n",
    "        key = (ppl * 10 + 1, ppl * 10 + 11)\n",
    "        raw_canaries = llama_tokenizer.batch_decode(non_members[seq_len][key][:, 1:])\n",
    "\n",
    "        df_res_tmp = compute_perplexity_df(\n",
    "            llama_model=llama_model,\n",
    "            croissant_model=croissant_model,\n",
    "            llama_tokenizer=llama_tokenizer,\n",
    "            croissant_tokenizer=croissant_tokenizer,\n",
    "            raw_canaries=raw_canaries,\n",
    "        )\n",
    "        df_res_tmp[\"seq_len\"] = seq_len\n",
    "        df_res_tmp[\"ppl\"] = ppl\n",
    "        df_res_tmp[\"n_rep\"] = 0\n",
    "\n",
    "        df_res = pd.concat([df_res, df_res_tmp])\n",
    "\n",
    "    df_res[\"ratio\"] = df_res.croissant_ppl / df_res.llama_ppl\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(df_res, seq_len=100, n_rep=1000):\n",
    "    y_score, y_true = [], []\n",
    "    df_members = df_res[(df_res.seq_len == seq_len) & (df_res.n_rep == n_rep)]\n",
    "    score_members = np.log(df_members[\"croissant_ppl\"]) / np.log(df_members[\"llama_ppl\"])\n",
    "    y_score.extend(list(score_members))\n",
    "    y_true.extend([1] * len(score_members))\n",
    "\n",
    "    df_non_members = df_res[(df_res.seq_len == seq_len) & (df_res.n_rep == 0)]\n",
    "    score_non_members = np.log(df_non_members[\"croissant_ppl\"]) / np.log(df_non_members[\"llama_ppl\"])\n",
    "    y_score.extend(list(score_non_members))\n",
    "    y_true.extend([0] * len(score_non_members))\n",
    "\n",
    "    auc = roc_auc_score(y_true, -np.array(y_score))\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = (5, 25, 45, 65, 85, 105, 125, 145, 165, 190)\n",
    "all_aucs = defaultdict(list)\n",
    "\n",
    "for seq_len in (25, 50, 100):\n",
    "    for checkpoint in tqdm(checkpoints):\n",
    "        checkpoint_name = f\"croissantllm/base_{checkpoint}k\"\n",
    "        croissant_model = AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint_name, cache_dir=\"/home/igor/rds/ephemeral/.huggingface\").to(\"cuda:0\")\n",
    "\n",
    "        df_res = make_df_res(croissant_model)\n",
    "\n",
    "        auc = compute_auc(df_res=df_res, seq_len=seq_len)\n",
    "        all_aucs[seq_len].append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5), sharey=True)\n",
    "\n",
    "# Define custom color combinations\n",
    "custom_colors = ['darkred', 'darkblue', 'darkgreen']\n",
    "\n",
    "for j, seq_len in enumerate((25, )):\n",
    "    aucs_to_plot = all_aucs[seq_len]\n",
    "    axes.plot(checkpoints[:len(aucs_to_plot)], aucs_to_plot,\n",
    "              label=\"$L_{ref}$ = \"+str(seq_len), marker=\"o\", markersize=8,\n",
    "              alpha=1, linewidth=2, color=custom_colors[j], markeredgecolor='white', markeredgewidth=2)\n",
    "\n",
    "# axes.set_title(\"Measuring memorization during training\", fontsize = 13)\n",
    "axes.axhline(0.5, linestyle='--', linewidth=2, c='black', alpha=1, label='Random guess baseline')\n",
    "\n",
    "axes.set_xlim(0, 200)\n",
    "axes.set_xticks(np.arange(0, 201, 20))\n",
    "axes.set_xticklabels(np.arange(0, 201, 20))\n",
    "axes.set_xlabel(\"Training steps (in thousands)\", fontsize=15)\n",
    "axes.set_ylabel(\"AUC\", fontsize=15)\n",
    "axes.set_ylim(0.3, 0.9)\n",
    "axes.grid()\n",
    "axes.legend(loc='upper left', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
